\chapter{LITERATURE REVIEW}
\section{Theoretical Basis}
  \subsection{Object Detection Task}

  Object detection is a fundamental task in computer vision.
  It is a task that have 2 objective, localization and classification.
  Localization is about determining the spatial properties of an object in an image. E.g. by
  enclosing objects in bounding boxes, determining which part of the image is an object and which part is not. 
  On the other hand, classification is about determining the class of
  objects. An object detection algorithm has to determine the label of an object whether
  they are an animal, human, necromancer, or something else in the set of classes.

  To measure how well an object detection algorithm or model perform, computer vision
  researchers defined metrics to measure how fit a model prediction to an object
  and to measure how well the model perform across all objects. Example of those metrics are Intersection Over Union (IoU) and Mean Average Precision (mAP).
  
  %\lipsum[1-6]



  \subsubsection{Intersection Over Union}
   \begin{figure}[p]
        \centering
        \begin{subfigure}[][][t]{\textwidth}
          \centering
          \includegraphics[width=.33\linewidth]{figures/iou-intersect-partly.png}
          \caption{Bounding Boxes Partly Intersect}
          \label{fig:iou-intersect-partly}
        \end{subfigure}

        \begin{subfigure}[][][t]{.49\textwidth}
          \centering
          \includegraphics[width=.66\linewidth]{figures/iou-intersect-fully.png}
          \caption{Bounding Boxes Fully Intersect}
          \label{fig:iou-intersect-fully}
        \end{subfigure}\hfill
        \begin{subfigure}[][][t]{.49\textwidth}
          \centering
          \includegraphics[width=.66\linewidth]{figures/iou-intersect-inside.png}
          \caption{Bounding Box Inside Another Bounding Box}
          \label{fig:iou-intersect-inside}
        \end{subfigure}
        \caption{Three Cases of Bounding Boxes Intersection}
        \label{fig:intersections-iou}
    \end{figure}
    \begin{figure}[p]
        \centering
        \includegraphics[width=0.9\textwidth]{figures/inter-union.png}
        \caption{Important Points For IoU Calculation}
        \label{fig:inter-union}
    \end{figure}
  

  IoU is a widely used metrics to determine how fit a predicted bounding box against the true bounding box.
  It is done by calculating the area of the intersection between the predicted and dividing it by the area of the
  union of those 2 boxes. 
  This way, IoU is robust to objects of different sizes and aspect ratio. 

  Bounding box of an object can be large, small, wide, or tall.
  Since IoU consider the intersection of bounding boxes in relation to the union,
  IoU provides a normalized measure of the fitness of boxes, making it robust 
  to the bounding boxes size. For example, if 2 boxes A and B intersect, there would be
  three possible cases, A fits B perfectly, A is inside B or vice versa, and some of A in B or vice versa as shown in Figure \ref{fig:intersections-iou}.
  When bounding box A fits B perfectly, the value of IoU would be 1 as the area of intersection is the same as the area of union.
  For the case where a bounding box is inside another, IoU can lower the score if the enclosed box's size is smaller compared to
  the enclosing box due to the normalization by the union area.
  For the case when the boxes partly intersect, the area of intersection and area of union would be suboptimal, leading to a lower IoU score. 
  We can say that the intersection part of the IoU provide a score for the bounding boxes position similarities and the 
  union provide score for size similarities.

 
  
  %The IoU of 2 bounding boxes A and B can be calculated using the following way:
  To calculate the IoU of two bounding boxes A and B, the area of intersection and union must first
  be calculated. The following shows the process of calculating IoU.
  \begin{itemize}[topsep=0pt]
    \item Calculate the area of intersection of A and B. 
    Let $(x_A,y_A)$ and $(x_B,y_B)$ be coordinates of the center of the boxes A and B respectively,
    and let $(w_A,h_A)$ and $(w_B,h_B)$ be widths and heights of boxes A and B, as demonstrated in Figure \ref{fig:inter-union}.

    The calculation for the area of intersection can be done like this:
    \begin{align*}
      x_{\text{{inter}}} &= \max(x_A, x_B) \\
      y_{\text{{inter}}} &= \max(y_A, y_B) \\
      w_{\text{{inter}}} &= \min(x_A + w_A, x_B + w_B) - x_{\text{{inter}}} \\
      h_{\text{{inter}}} &= \min(y_A + h_A, y_B + h_B) - y_{\text{{inter}}} \\
      \text{Area}_{\text{inter}} &= w_{\text{{inter}}} \times h_{\text{{inter}}}
    \end{align*}

    \item Calculate the area of union.

    From set theory, we know that $|A \cup B| =|A| + |B| - |A \cap B| $.
    This equation also applies on calculating the area of union.

    \begin{align*}
      \text{Area}_{union} &= \text{Area}_A + \text{Area}_B - \text{Area}_{\text{inter}}\\
      \text{Area}_{union} &= w_A\times h_A + w_B\times h_B  - \text{Area}_{\text{inter}}
    \end{align*}

    \item Finally, calculate IoU

    \begin{equation}
      IoU = \frac{\text{Area}_{\text{inter}}}{\text{Area}_{\text{union}}}
    \end{equation}
  \end{itemize} 

  

  \subsubsection{Mean Average Precision}
  Average Precision (AP) is a popular metrics used to measure the capability of an object detection model
  for a given dataset. The main advantage of using AP are its ability to capture the precision recall
  tradeoff and its independence towards confidence threshold. AP has these 2 advantage as an effect of the way 
  it is calculated.

  Calculating AP is the same as calculating area under the curve (AUC) of precision and recall curve.
  Precision is defined as
  \begin{equation}
    P(\mathbb{S}_c,h,\tau,\epsilon) = \dfrac{\left|\left\{(\hat{B},x) \in \mathbb{S}_c :\ \exists B \in h(x,\tau)_c,\ IoU(\hat{B},B) > \epsilon  \right\}\right|}{\displaystyle\sum_{x\in \mathbb{S}_{x,c}} |{B \in h(x,t)_c}|}
    \label{eq:precision}
  \end{equation}
  \begin{align*}
    \text{Where}~ h &=  \text{hypothesis/model that predict bounding boxes}\\%\tau}\\
    \mathbb{S}_c &= \text{set of bounding boxes of class $c$ in dataset paired with their respective image}\\
    h(x,\tau)_c &= \text{bounding boxes predicted by $h$ with class $c$}\\
    \tau &= \text{confidence threshold for $h$} \\
    \epsilon &= \text{$IoU$ threshold for a positive prediction}\\
    B &= \text{Bounding box}\\
    x &= \text{image}
  \end{align*}
  and for recall, it is defined as
  \begin{equation}
    R(\mathbb{S}_c,h,\tau,\epsilon) = \dfrac{\left|\left\{(\hat{B},x) \in \mathbb{S} :\ \exists B \in h(x,\tau)_c,\ IoU(\hat{B},B) > \epsilon  \right\}\right|}{\left| \mathbb{S}_c \right|}
    \label{eq:recall}
  \end{equation}
  Then we define the precision recall curve as a parametric function
  \begin{equation}
    PR(\tau) = \left( R(\mathbb{S}_c,h,\tau,\epsilon),P(\mathbb{S},h,\tau,\epsilon) \right)
    \label{eq:pr}
  \end{equation}
  Using equation \ref{eq:pr}, $PR$ points in Figure \ref{fig:pr-curve} can be generated using $0 \leq \tau \leq 1$
  \begin{figure}[p]
        \centering
        \includegraphics[width=0.7\textwidth]{figures/pr-curve.png}
        \caption*{Source: \textcite{map-hui} under CC BY 4.0}
        \vspace{-2ex}
        \caption{PR Points Generated by Varying $\tau$}
        \label{fig:pr-curve}
  \end{figure}
  Before calculating the AP however, the curve in Figure \ref{fig:pr-curve} is usually interpolated using Equation \ref{eq:pr-inter},
  which relies on Equation \ref{eq:pr-functionization} that transformed $PR$ curve to a functional relation of R to P.
  The interpolated curve can be seen on \ref{fig:pr-interp}
  \begin{align}
    \label{eq:pr-inter}
    p_{inter}(r) &= \max_{\bar{r}>r} p(\bar{r})\\
    \label{eq:pr-functionization}
    \text{Where}~p(\bar{r}) &= \max \left\{p : \forall (r,p) \in \{PR(\tau) : 0\leq\tau\leq 1\}\ \land r=\bar{r}\right\}%\text{precision given recall value in Figure \ref{fig:pr-curve}}
  \end{align}

  \begin{figure}[p]
        \centering
        \includegraphics[width=0.7\textwidth]{figures/pr-interp.png}
        \caption*{Source: \textcite{map-hui} under CC BY 4.0}
        \vspace{-2ex}
        \caption{PR Curve Interpolated}
        \label{fig:pr-interp}
  \end{figure}
  The AP, which is the area under the curve then can be calculated using the following integral:
  \begin{equation}
    AP = \int_{0}^{1} p_{inter}(r) \, dr
  \end{equation}

  When calculating AP, the $IoU$ threshold for positive detection is usually predefined. As example,
  for AP@50, we set the $\epsilon$ in Equation \ref{eq:precision} and \ref{eq:recall} to 50\%. And for 
  AP@75, the $\epsilon$ is set to 75\%.

  The AP calculation so far only works for a single class. To have a multiclass metric, we use mAP
  which is calculated as the average AP across all classes in the dataset.
  \begin{equation}
    mAP@X = \frac{1}{|\text{classes}|} \sum_{c\in \text{classes}} AP_c@X
  \end{equation}
  %\begin{align*}
  %  \text{Where}~p(r) &= \text{precision given recall value in Figure \ref{fig:pr-curve}}
  %\end{align*}
  %Traditionally, object detection algorithms relied on handcrafted feature kernels and machine learning techniques. 
  %However, deep learning has become a popular solution for object detection by the use of Convolutional Neural Networks (CNN)
  %to learn feature kernels automatically. With the usage of deep learning, object detection task
  %had significant advancement in terms of accuracy and efficiency.

  \subsection{Deep Learning}
  \subsubsection{Neural Network}
  Neural Network are computational models that drew inspiration from how neural network in brain works.
  It imitates a simplification model of human brain which consist of large number of simple computational block (neuron)
  that are connected to each other, but able to carry out complex computation \parencite{mltheorytoalgo}. 

  A neural network can be described as directed graph where each node of the graph is a neuron and the edges are the link between the neurons.
  Each neuron will take the weighted sum of the output of other neurons that are connected it via incoming edge, and will output a value that would be an input to another neuron.
  We can describe a neural network with a graph $G = (V,E)$ and a weight function $w : E \rightarrow \mathbb{R}$, where each node is modeled as activation function $\sigma : \mathbb{R} \rightarrow \mathbb{R}$.

  To simplify explanation here like how \textcite{mltheorytoalgo} did, we assume the neural network form a feed-forward neural network (DAG structure) and is organized in layers.
  We can say that each node in $V$ can be decomposed as a union non-empty disjoint subsets of nodes, $V = \coprod_{t=0}^T V_t$, such that
  every edge in $E$ connect some node in $V_{t-1}$ to $V_t$ for $t\in [T]$.

  We call $V_0$, the first layer of the neural network as the input layer. It has $n_0+1$ nodes where $n_0$ is the dimensionality of input space.
  For every $i$ in $[n_0]$, the output of neuron $i$ is just $x_i$, the values of input vector $\vec{x}$. The last neuron in $V_0$ is a constant neuron that always outputs 1.
  We denote $v_{t,i}$ to be the $i$-th neuron of the $t$-th layer and $o_{t,i}(\vec{x})$ as the output of that neuron when the neural network is fed with input vector $\vec{x}$.

  Now we can perform the neural network calculation in layer-by-layer manner.
  Let $a_{t+1,j}(\vec{x})$ be the input to $v_{t+1,j}$ when the network is fed with $\vec{x}$.
  We can calculate $a$ with the following:
  \begin{equation}
    a_{t+1,j}(\vec{x}) = \sum_{r : (v_{t,r}, v_{t+1,j}) \in E} w((v_{t,r}, v_{t+1,j}))o_{t,r}(\vec{x}),
    \label{eq:forward-equation}
  \end{equation}
  and $o$ can be calculated with:
  \begin{equation}
    o_{t+1,j}(\vec{x}) = \sigma(a_{t+1,j}(\vec{x}))
    \label{eq:forward-activation}
  \end{equation}

  Equation \ref{eq:forward-equation} shows how the input of the neuron in the next layer is the weighted sum of the neurons in the previous layer that is connected to that neuron.
  The weighted sum are regulated according to $w$, and Equation \ref{eq:forward-activation} shows how the output of a neuron $v_{t+1,j}$ is only the application of activation function $\sigma$ on its input.

  The layers beside the input layer $V_0$ and output layer $V_T$ are often called hidden layers.
  $T$, the number of layers in the neural network, sometimes referred to as the depth of the network, hence the name deep learning.
  $|V|$ is called the size of the network and $\max_t |V_t|$ is called the width of the network.
  
  An illustration of the layers in a neural network can be seen on Figure \ref{fig:neuralnets}
  The illustrated neural network has a depth of 2, size of 10, and width of 4.

  \begin{figure}
      \centering
      \includegraphics[width=.9\textwidth]{figures/neuralnets.png}
      \caption{Illustration of neural network with $T=2$}
      \label{fig:neuralnets}
  \end{figure}

  \subsubsection{Neural Network as Hypothesis Class}
  Previously, we have specified a neural network architecture using $V$, $E$, and $\sigma$.
  This architecture is a hypothesis class or model class that consist of functions $h_{V,E,\sigma,w} : \mathbb{R}^{|V_0|-1} \rightarrow \mathbb{R}^{|V_T|}$.
  With $V$, $E$, and $\sigma$ fixed, we can define the hypothesis class of the neural network architecture by:
  \begin{equation}
    \mathcal{H}_{V,E,\sigma} = \{h_{V,E,\sigma,w} : w \text{ is a mapping of } E \rightarrow \mathbb{R} \}
    \label{eq:nn-hypot-class}
  \end{equation}

  Equation \ref{eq:nn-hypot-class} means that to find a model in the hypothesis class $\mathcal{H}$, we can do so by specifying the
  weights of the edges $w$.
  
  \subsubsection{Neural Network Learning}
  The learning of a neural network is the searching for the best hypothesis/model from hypothesis class $\mathcal{H}$.
  To find a model in $\mathcal{H}_{V,E,\sigma}$ with a low risk over the data by tuning $w$, we can apply
  some heuristic search approach by stochastic gradient descent (SGD).
  %For simpler explanation, $\sigma$ is assumed to be sigmoid function.

  We can think of the weight function $w$ as a vector $\vec{w} in \mathbb{R}^|E|$.
  If $n=|V_0|-1$ the number of input neurons and $k=|V_T|$ the number of output neurons,
  we can call a hypothesis defined by $\vec{w}$ in the hypothesis class to be a function $h_{\vec{w}} : \mathbb{R}^n \rightarrow \mathbb{R}^k$.
  Let $\Delta(h_{\vec{w}}(\vec{x}),\vec{y}) $ be the loss of predicting $h_{\vec{w}}(\vec{x})$ when the target label is $\vec{y}$.
  The risk of the network over data distribution $\mathcal{D}$ can be defined as:
  \begin{equation}
    L_{\mathcal{D}}(\vec{w}) = \mathop{{}\mathbb{E}}_{(\vec{x},\vec{y}) \sim \mathcal{D}} [\Delta(h_{\vec{w}}(\vec{x}),\vec{y})]
    \label{eq:risk}
  \end{equation}

  Now, we can use SGD to do empirical risk minimization (ERM) according to risk in Equation \ref{eq:risk}.
  The ERM can be done by setting the $\vec{w}$ to be a random vector, and perform SGD with the data.
  The gradients of each element in $\vec{w}$ however does not have a closed form. Instead, it is calculated using backpropagation.
  Sometimes it is also useful to add regularization term to the risk equation, that is, we try to minimize
  $L_{\mathcal{D}} + \frac{\lambda}{2} ||\vec{w}||^2$ for better result.


  \subsection{YOLO Family Architecture}
  YOLO is an abbreviation of "You Only Look Once" which describes what kind of neural network YOLO
  is, a single stage object detector. It means that this architecture predicts regions 
  and classes both at once. In contrast, two-stage detector predicts objects' regions first
  and then predicts their classes. Detecting objects in a single-stage manner is what gave YOLO
  architecture the ability to infer in real-time. This is possible due to how YOLO architecture
  was designed. YOLO architecture consist of 3 main part, the head, the neck, and the backbone.

    %Arsitektur famili YOLO pada dasarnya terbagi akan 3 bagian yaitu \emph{head}, \emph{neck}, dan \emph{backbone}.
    %Setiap bagian ini mempunyai fungsi masing-masing.
   
    %Berikut adalah penjelasan fungsi dan cara kerja dari ketiga bagian tersebut.
    \subsubsection{Backbone}
    The backbone is the network that extract features from the inputted image.
    Typically, the backbone is composed of deep neural network layers that progressively 
    down sample the spatial dimensions of the input while increasing the number of 
    learned features or meaningful abstraction of the data.

    The implementation of backbone in YOLO usually varies from one version to another.
    As example, \textcite{yolov2}'s YOLOv2 implemented Darknet-19 network as backbone, 
    \textcite{yolov3}'s YOLOv3 implemented Darknet-53, \textcite{yolov4}'s YOLOv4
    with their CSP-Darknet-53, and \textcite{vityolo} with their non-CNN (Transformer) backbone.
    Each of these network has their own has their own advantages and disadvantage when
    it comes to accuracy, memory requirement, or latency.


    %\emph{Backbone} dari YOLO merupakan bagian yang mengekstrak fitur dari citra yang diinputkan.
    %Hasil ekstraksi fitur ini akan diinputkan pada \emph{neck} yang kemudian akan di\emph{upsampling} olehnya.
    %Model-model YOLO dapat menggunakan \emph{feature extractor} dari model-model klasifikasi citra sebagai \emph{backbone}-nya.
    %Sebagai contoh, salah satu varian YOLO, YOLO-Z menggunakan DenseNet sebagai \emph{backbone}-nya sedangkan arsitektur YOLO dasarnya, YOLOv5 menggunakan \emph{backbone} YOLOv5v7.0 \parencite{yoloz}.
    \subsubsection{The Neck}
  
    \begin{figure}
        \centering
        \includegraphics[scale=0.55]{figures/yolo-architecture-rough.png}
        \caption{Feature Pyramid Network in YOLOv3}
        \label{fig:yolofpn}
    \end{figure}

    The neck is the intermediate network between backbone and head and its
    main function of the neck is to enhance features extracted by the backbone.
    The specific implementation of neck for each YOLO architecture is different one to the other, however.
    One common approach is to combine feature maps from different prediction scales of the YOLO network.
    %Some neck implementation try to combine feature maps across different prediction scales of YOLO network.

    In YOLOv3, \textcite{yolov3} introduced prediction in multiple scale.
    To utilize information across scales, YOLOv3 fuses features 
    from multiple parts of the backbone before feeding them to the head as seen on Figure \ref{fig:yolofpn} 
    by using Feature Pyramid Network (FPN) as its neck. 

    A further improvement was made by \textcite{yolov4} with their YOLOv4 by introducing Path 
    Aggregation Network (PANet) for the neck. 
    With PANet, feature are fused back to higher scale by adding another FPN-like layer after the original FPN
    but with reverse direction.  This way, the features fused not only top to bottom but also bottom to top, making top feature have information learned by bottom feature and vice versa. 
    This result in a better integration of multiscale information and increased the object detection accuracy.

    Overall, the neck's role in YOLO architectures is to facilitate feature enhancement and fusion across different scales to improve detection performance.
    %The way it works is by taking feature maps, not only in the last output layer of the
    %backbone, but also in multiple parts of the backbone.
    %For example, YOLOv4 uses PANet to enhance and combine features across different scales
    %of prediction.
  
    %\emph{Neck} dari YOLO merupakan \emph{layer-layer} dimana \emph{head} YOLO mengambil fitur untuk dilakukan deteksi \emph{bounding box}.
    %Pada YOLOv3 \textcite{yolov3}, arsitektur \emph{neck} dibuat menyerupai \emph{Feature Pyramid Network} (FPN) seperti pada Gambar \ref{fig:yolofpn}. 
    %Pada versi-versi YOLO selanjutnya, bentuk \emph{neck} ini tidak banyak berubah dan pada dasarnya tetap mempertahankan bentuk \emph{pyramid}-nya.
  
    %Penaikkan tingkatan \emph{pyramid} dari FPN merupakan \emph{upsampling} dari \emph{feature map} yang dihasilkan \emph{backbone}.
    %Output tiap tingkatan pada FPN di \emph{neck} inilah yang diinputkan pada \emph{head} YOLO. 
    %Melakukan prediksi pada tingkatan \emph{upsampling} yang berbeda-beda dapat membuat \emph{neural network} mendapatkan lebih banyak informasi semantik dan informasi yang lebih detail sehingga dapat lebih akurat dalam mendeteksi objek besar maupun kecil.
 

  
    \subsubsection{The Head and The Anchors}

    The head is where the object detection happens. Extracted and enhanced features of the image is fed to the head 
    in multiple different scales. For each scale, the head will predict a box for each $N_k \times N_k$
    lattice point on feature map's grid. In total, each layer will output a tensor with size
    $N_k \times N_k \times [A_k \times (4+1+C)]$ where $N_k$ is the size of feature map grid at the $k$-th scale,
    $A_k$ is the number of anchors for that scale, 4 is for the four offsets $[t_x, t_y, t_w, t_h]$ (see Figure 
    \ref{fig:anchorbox}), 1 is for the objectness score for the grid, and $C$ is for the number of classes it has
    to predict.
    
    Most of YOLO family architecture head utilizes anchor boxes to assist bounding box prediction.
    This technique is used in \textcites{yolov2}{yolov3}{yolov4}{scaledyolov4}{yolov5}{yolor}{yolov7}.
    Instead of directly predicting the size and position of the bounding box, YOLO head predicts 
    the offsets for each anchor boxes assigned to the head, then it utilizes the objectness score to pick
    which result  of these anchor boxes to be used.
    Using anchor boxes allows the neural network to converge more quickly because it provides
    a prior knowledge of the dataset before training.
    
    \begin{figure}[t]
        \centering
        \includegraphics[width=0.45\textwidth]{figures/anchor.pdf}
        \caption{Prediction for Anchor Box and Its Offsets from Lattice of The Feature Grid }
        \label{fig:anchorbox}
    \end{figure}

    %The way it works is that, there will be a preset of detection boxes.
    %Arsitektur famili YOLO yang dipublikasikan setelah YOLOv2 terus menggunakan \emph{anchor box} untuk melakukan deteksi \parencites{yolov2}{yolov3}{yolov4}{scaledyolov4}{yolov5}{yolor}{yolov7}.
    %\emph{Anchor boxes} merupakan beberapa \emph{Bounding Box} yang telah terdefinisikan. 
    %Arsitektur YOLO akan memprediksi probabilitas \emph{anchor box} berada pada suatu koordinat latis beserta dengan \emph{offset anchor box} tersebut untuk menepatkan \emph{anchor box} pada objek yang dideteksi.
    %Penggunaan \emph{anchor box} ini dapat meningkatkan akurasi deteksi karena \emph{neural network} hanya perlu mencari titik tengah objek dan \emph{error} dimensi \emph{boudning box} dengan menggunakan \emph{offset} \parencite{yolov3}.
    %Hal ini lebih sederhana daripada mencari titik-titik \emph{bounding box} secara independen sehingga lebih mudah untuk dipelajari oleh \emph{neural network}.
  
    %Prediksi \emph{bounding boxes} terjadi di bagian \emph{head} dari arsitektur YOLO.
    %Bagian \emph{head} dari YOLO akan mengambil beberapa hasil \emph{upsampling} yang terjadi pada \emph{neck} YOLO, dan kemudian melakukan prediksi \emph{anchor boxes} dari hasil tersebut.
    %Hasil prediksi \emph{Head} YOLO pada suatu tingkatan \emph{upsampling} berupa tensor dengan ukuran $N\times N \times [A\times(4+1+C)]$ dengan $N$ sebagai dimensi hasil \emph{upsampling}-nya, $A$ sebagai jumlah \emph{anchor boxes} untuk \emph{scaling} tersebut, dan $C$ sebagai jumlah kelas prediksi.
    %Angka 4 merepresentasikan 4 \emph{offset} $b_x, b_y, b_w, b_h$ seperti pada Gambar \ref{fig:anchorbox} dan angka 1 merepresentasikan \emph{objectness score} dari prediksi \emph{bounding box}.

    \subsubsection{Loss Function}
    The goal of a YOLO architecture is to (1) predict if an object exist or not, (2) predict the bounding box of such object,
    and (3) predict the class of the object. These 3 loss functions that correspond to those objectives are called 
    objectness loss, box loss, and class loss respectively. To update the weights on training, the total loss is calculated as 
    the weighted sum of those 3 losses.
    %\begin{equation}
    %  L_{box} = \sum_{k=0}^{n}\sum_{i,j=0}^{N_k}\sum_{m=0}^{A_k} \mathbbm{1}_{k,i,j,m}^{obj} -IoU(gt(x), M(x)_{k,i,j,m})
    %\end{equation}
    In original YOLO, the loss functions were defined like the following.
    For localization loss, it is described by equation \ref{eq:yolo-box-loss}.
    \begin{equation}
      L_{box} = \sum_{i=0}^{S^2} \sum_{j=0}^B \mathbbm{1}_{ij}^{obj} \left[(x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2 + (\sqrt{w_i}-\sqrt{\hat{w}_i})^2 + (\sqrt{h_i}-\sqrt{\hat{h}_i})^2\right] \\
      \label{eq:yolo-box-loss}
    \end{equation}
    \begin{align*}
    \text{Where}~S^2  &= \text{the total number of grid cells in the output,}\\
    B &= \text{ the total number of anchor box per grid cells,}\\
    \mathbbm{1}_{ij}^{obj} &= \begin{cases}
                                1, & \text{if object present in grid}\\
                                0, & \text{otherwise,}
                              \end{cases} 
                              \\
    (x_i, y_i) &= \text{the predicted coordinates of the center of object i}\\
    (\hat{x}_i, \hat{y}_i) &= \text{the ground truth coordinates of the center of object}\\
    (w_i, h_i) &= \text{the predicted width and height of object i}\\
    (\hat{w}_i, \hat{h}_i) &= \text{the ground truth width and height of object}
      %\text{the indicator variable that has value 1 if object is present in the grid and 0 otherwise.}
    \end{align*}
    %$\mathbbm{1}_{ij}^{obj}$ is the indicator variable that has value 1 if object is present in the grid and 0 otherwise.
    %$(x_i, y_i)$ and $(\hat{x}_i, \hat{y}_i)$ are the predicted and ground truth coordinates of the center of object $i$.
    %$(w_i, h_i)$ and $(\hat{w}_i, \hat{h}_i)$ are the predicted and ground truth widths and heights of object $i$.
    For objectness loss, described by equation \ref{eq:yolo-obj-loss}
    \begin{equation}
      L_{obj} = \sum_{i=0}^{S^2} \sum_{j=0}^B \mathbbm{1}_{ij}^{obj}(C_i - \hat{C}_i)^2  + \alpha \mathbbm{1}_{ij}^{noobj}(C_i - \hat{C}_i)^2
      \label{eq:yolo-obj-loss}
    \end{equation}
    \begin{align*}
    Where~\mathbbm{1}_i^{\text{noobj}} &=\begin{cases} 
                                          1, & \text{if object assigned to anchor j}\\
                                          0, & \text{otherwise} 
                                         \end{cases}\\
          (C_i,\hat{C}_i) &= \text{predicted and ground truth confidence score for objectness of anchor}\\
          \alpha &= \text{weight to punish false positives}
    \end{align*}
    %$(C_i, \hat{C}_i)$ are the predicted and ground truth confidence scores for objectness of anchor $i$,
    And for class loss, described by \ref{eq:yolo-class-loss}
    \begin{equation}
      L_{class} = \sum_{i=0}^{S^2} \mathbbm{1}_{ij} \sum_{c \in classes} (p_i(c) - \hat{p}_i(c))^2
      \label{eq:yolo-class-loss}
    \end{equation}
    Where $(p_i(c), \hat{p}_i(c))$ are the predicted and ground truth class probabilities for class $c$ for object $i$.

    The three losses combined to the final loss function in equation \ref{eq:yolo-loss}
    \begin{equation}
      L = \lambda_{box}L_{box} + \lambda_{obj}L_{obj} + \lambda_{class}L_{class}
      \label{eq:yolo-loss}
    \end{equation}
    \begin{align*}
      Where~\lambda_{box} &= \text{the weight for localization,}\\
      \lambda_{obj} &= \text{weight for objectness}\\
      \lambda_{class} &= \text{weight for class}
    \end{align*}
    These 3 $\lambda$-s can be tuned to optimize the performance of a YOLO network.

    %\begin{equation}
    %  L_{box} = \sum_{i=0}^{S^2}\sum_{j=0}^B   \mathbbm{1}_{ij}^{obj} (x_i-\hat{x}_i)^2 + (y_i-\hat{y}_i)^2 + (\sqrt{w_i}-\sqrt{\hat{w}_i})^2 + (\sqrt{h_i}-\sqrt{\hat{h}_i})^2 
    %\end{equation}
      %L_{box} = \sum_{k=0}^{n}\sum_{i,j=0}^{N_k}\sum_{m=0}^{A_k} \mathbbm{1}_{k,i,j,m}^{obj} -IoU(gt(x), M(x)_{k,i,j,m})

    %\begin{equation}
    %  L_{box} = 1
    %\end{equation}

  
      
  \subsection{YOLOv7}
  As said in section \ref{section:background}, YOLOv7 is the state-of-the-art real-time object detector.
  It was made by the authors of YOLOv4, and by the time it was published (July 2022), it surpassed all 
  known real-time object detectors both in speed and accuracy. To achieve this, YOLOv7 implemented some new
  changes and addition to the neural network. 
  \subsubsection{Backbone}

%  \vspace{-2ex}
  %\begin{figure}[H]
  %    \centering
  %    \subfigure[ELAN block]{
  %    \includegraphics[width=0.2\textwidth]{figures/elan-block.png}
  %    \label{fig:elan-block}
  %    }
  %    \subfigure[FIGTOPCAP][First two ELAN blocks in YOLOv7]{
  %    \includegraphics[width=0.75\textwidth]{figures/yolo-elan-blocks.png}
  %    \label{fig:elan-yolo}
  %    }
  %    \caption{ELAN in YOLOv7}
  %    \label{fig:elan}
  %\end{figure}
  \begin{figure}[p]
    \centering
    \begin{subfigure}[b]{.5\textwidth}
        \centering
        %%%\vspace{0pt}
        \includegraphics[width=.8\linewidth]{figures/elan-block.png}
        \caption*{Source: \textcite{yolov7}}
        \vspace{-2ex}
        \caption{ELAN block}
        \label{fig:elan-block}
    \end{subfigure}%
    \begin{subfigure}[b]{.5\textwidth}
        \centering
        \includegraphics[width=.8\linewidth]{figures/yolo-elan-blocks-90.png}
        \caption{First two ELAN blocks in YOLOv7}
        \label{fig:elan-yolo}
    \end{subfigure}
    \caption{ELAN in YOLOv7}
    \label{fig:elan}
  \end{figure}

  %remove this negative vspace if buku TA kurang panjang

  YOLOv7 implemented the Efficient Layer Aggregation Network (ELAN) and Extended-ELAN (E-ELAN) as the backbone of its network. 
  ELAN is a convolutional neural network that was designed to extract features efficiently 
  by controlling the shortest longest gradient path in the network \parencite{elan}.
  This choice of backbone allows YOLOv7 to perform prediction more accurately despite having fewer number of parameters. 

  Figure \ref{fig:elan-yolo} shows how ELAN block in Figure \ref{fig:elan-block} implemented in YOLOv7.
  %After 3 convolution, the [640x640x3] input  propagate through the first ELAN block. Then, it's dimension was downsampled.
  First, the $640\times 640\times 3$ input $x$ propagate through 3 convolution layers, down-sampling the dimension to $160\times 160$ and increased the channels to $128$.
  Then it encounter the first ELAN block. ELAN block did not change the dimension, but it doubled the number of channels, making the dimension of $x$ to be $160\times 160\times 256$.
  Before propagating $x$ to the next ELAN layer however, $x$ is down-sampled to $80\times 80\times 256$ using max-pooling and convolution layers. 
  Prior to each ELAN layers in the backbone, the feature will be down-sampled in its width and height dimension by half.

  \subsubsection{Label Assignment Strategy and Auxiliary Head}

  SimOTA, first introduced in YOLOX, is an algorithm to approximate Optimal Transport Assignment (OTA)
  in a faster way. \textcite{yolox} introduced SimOTA in YOLOX because OTA was deemed too slow to compute
  as it was increasing the training time by 25\%. YOLOv7 also implemented SimOTA for its dynamic label assigner.

  YOLOv7 deep supervised its training process by attaching auxiliary heads to its neural network
  as seen on Figure \ref{fig:aux-head}.
  These auxiliary heads are only used on training, on inference, they are removed from the neural network
  to improve latency, only the lead head is kept. There is a problem however with assigning labels
  to the auxiliary and lead heads. Most object detection networks that utilizes auxiliary heads have 2
  independent label assigners, one for auxiliary heads and one for lead heads. YOLOv7 done things differently.


  YOLOv7 proposed 2 way of assigning labels to auxiliary and lead heads. Lead head guided label assignment (Figure \ref{fig:lead-head}) and
  coarse-to-fine lead head guided assignment (Figure \ref{fig:coarse-to-fine}). For lead head guided label assignment, the assigner gives a copy
  of lead heads' label assignment to the auxiliary heads. For coarse-to-fine lead head guided assignment, the 
  assigner works like lead head guided assigner but gives coarse label assignment to auxiliary head. Coarse label
  assignment is done by relaxing the positive sample constraints of the assigner.   

  Due to the relaxed constraints, coarse label assignment to auxiliary heads assigns more positive labels the auxilary heads' grids. 
  This way, the network will learn more to recall.
  On inference, this recall ability would be filtered by the lead head to produce accurate prediction.
  \textcite{yolov7} find that coarse-to-fine
  label assignment produces the greatest AP scores.

  \begin{figure}[t!]
      \centering

      \begin{subfigure}[][][t]{\textwidth}
        \centering
        \includegraphics[width=.33\linewidth]{figures/auxilary-head.png}
        \caption{Auxiliary heads attachment in YOLOv7}
        \label{fig:aux-head}
      \end{subfigure}

      \begin{subfigure}[][][t]{0.49\textwidth}
        \centering
        \includegraphics[width=.66\linewidth]{figures/lead-head-assigner.png}
        \caption{Lead guided label assignment}
        \label{fig:lead-head}
      \end{subfigure}
      \begin{subfigure}[][][t]{0.49\textwidth}
        \centering
        \includegraphics[width=.66\linewidth]{figures/coarse-to-fine.png}
        \caption{Coarse-to-fine lead guided label assignment}
        \label{fig:coarse-to-fine}
      \end{subfigure}
      \caption*{Source: \textcite{yolov7} with permission (see Appendix \ref{appendix:license})} %TODO: ADD APPENDIX REFERENCE
        \vspace{-2ex}
      \caption{YOLOv7 Label Assignment Strategy with Auxilary Heads}
      \label{fig:labelassigner}
  \end{figure}

  \subsubsection{Reparameterization}
  YOLOv7 utilized RepConv and YOLOR implicit layers in its network.
  In addition to that, YOLOv7 also uses Convolution-BatchNorm layer.
  These layers can be reparameterized after training to simplify the neural network, thereby reducing
  latency and memory usage but not hurting inference performance. 

  The reparameterization of these layers can be done after training by computing
  some mathematical simplification of some layer combination.
  These combinations of layers are YOLOR\textsuperscript{+}--Convolution--YOLOR\textsuperscript{+}, 
  YOLOR*--Convolution--YOLOR*, and Convolution-BatchNorm.

  For combination of layers YOLOR\textsuperscript{+}--Convolution--YOLOR\textsuperscript{+} layers, it can be reparameterized like the following.
  \begin{align*}
    x_{n+1} &= W(x_{n}+g_1(z_1)) + b + g_2(z_2)\\
    &= W(x_{n}) + (W(g_1(z_1)) + b + g_2(z_2))\\
    &= W(x_{n}) + b'
  \end{align*}
  Observe that the reparameterization combined 3 layers into a single convolution layer with bias. 
  Then, For combination of YOLOR*--Convolution--YOLOR* layers:
  \begin{align*}
    x_{n+1} &= (W(g_1(z_1)x_n)+b)g_2(z_2)\\
    &= g_2(z_2)g_1(z_1)W(x_n) + b g_2(z_2)\\
    &= W'(x_n) + b'
  \end{align*}
  And finally for Convolution-BatchNorm:
  \begin{align*}
    x_{n+1} &= ((W(x_n)+b)-m)/s\\
    &= (W(x_n) + (b-m))/s\\
    &= (W/s)(x_n) + (b-m)/s\\
    &= W'(x_n) + b'
  \end{align*}

  In summary, YOLOR\textsuperscript{+}--Conv--YOLOR\textsuperscript{+} will be replaced with the original convolutional layer $W$ but with bias $W(g_1(z_1)) + b + g_2(z_2)$.
  YOLOR*--Conv--YOLOR* layers will be replaced with a convolutional $g_2(z_2)g_1(z_1)W$ and bias $b g_2(z_2)$.
  And Conv--BatchNorm will be replaced with a convolutional $W/s$ and bias $(b-m)/s$.
  %TODO: reparam equation here
    %YOLOv7 merupakan pendeteksi objek \emph{real time} dengan skor akurasi tertinggi pada dataset COCO di tahun 2022.
    %Pada YOLOv7, dilakukan beberapa perubahan untuk meningkatkan akurasi dan kecepatan deteksinya.
    %Perubahan-perubahan tersebut dilakukan pada arsitekturnya dan pada \emph{bag-of-freebies}-nya.
  
    %Perubahan arsitektur dilakukan pada \emph{backbone}. YOLOv7 menggunakan \emph{Extended Efficient Layer Aggregation Network} (E-ELAN) sebagai \emph{backbone}, berbeda dengan leluhurnya YOLOv4 yang menggunakan CSP-Darknet.
    %E-ELAN merupakan arsitektur \emph{neural network} yang efisien karena E-ELAN didesain dengan mengontrol \emph{gradient path} terpanjang yang terpendek.
    %Karena efisiensinya, arsitektur E-ELAN ini dapat meningkatkan kecepatan deteksi dan akurasi. \parencite{yolov7}
  
    %\emph{Bag-of-freebies} merupakan kumpulan metode peningkatan akurasi yang tidak meningkatkan \emph{cost inferrence} \parencite{yolov4}. 
    %Pada YOLOv7, ditambahkan beberapa \emph{bag-of-freebies} yang dapat dilatih seperti \emph{re-parameterized convolution} dan \emph{extra auxilary head} di tengah-tengah \emph{neural network}.
    %Selain kedua itu, YOLOv7 juga menambahkan \emph{trainable bag-of-freebies} dari YOLOR seperti EMA, \emph{Implicit Knowledge}, dan \emph{conv-bn topology Batch Normalization} \parencite{yolov7}.
    %Introducing YOLOv7, a state-of-the-art deep learning based real-time object detector \parencite{yolov7}.
    %At the time of the proposal for this research was made (November 2022),
    %YOLOv7 outperform both in speed and accuracy of all known real-time object detectors 
    %with inference speed in the range of 5-160 FPS. It also has the highest accuracy (56.8\% AP) among
    %object detectors with inference speed greater than 30 FPS on a V100 GPU. The capabilities of this cutting-edge architecture
    %makes it well-suited for AAV computer vision system. However, all the performance metrics of YOLOv7
    %mentioned before are obtained by training the model using COCO 2017 dataset. A dataset which 
    %consist of general objects that people see in their daily life. COCO dataset is going to have
    %very distinct distribution compared to airborne objects. As such, there would be a need for
    %some modification to YOLOv7 so that it could detect airborne objects well.


  \subsection{Anchor Recalculation}
  \label{section:anchor_recalc_study}
  Anchor recalculation is a common method of introducing prior distribution of the dataset to an anchor-based object
  detection networks. Thus, recalculating anchor can help the neural network learn faster.
  Most of the time, anchors provided by pretrained YOLO weights are optimized for the common metric
  dataset such as COCO2017 or VOC2012. 
  Therefore, recalculating anchors in a specific dataset can improve the neural network ability.

  There are multiple ways of recalculating anchors. Some of them can be performed before training or during training.
  Most of pre-training anchor recalculation method involves with clustering the anchors to the dataset. This is done by
  using clustering algorithm such as K-means, Gaussian Mixture, and many others. Recalculating anchor on training is 
  a little more complex to do as it will involve some loss function or architectural change on the object detection neural network.
  \textcite{anchoropt} for example add additional layer on detection part of object detectors that is connected to anchor modifiers such that
  the anchors will also be updated during training. \textcite{yolor} mentioned that the implicit multiplication layer of their network
  can be purposed for anchor refinement.

  %\emph{Anchor box} dari model-model \emph{pre-trained} YOLO pada umumnya mengoptimisasi \emph{anchor box} modelnya pada dataset COCO.
  %Ukuran \emph{anchor box} yang akan digunakan pada model YOLO dapat dikonfigurasikan agar lebih sesuai dengan dataset yang akan digunakan untuk melatih model YOLO.
  %Penyesuaian ini dapat meningkatkan IoU(\emph{Intersection Over Union}) prediksi model dengan \emph{ground truth} sehingga meningkatkan akurasi.

  %Penyesuaian anchor box dapat dilakukan pada saat sebelum training atau pada saat training.
  %Penyesuaian anchor box sebelum training dapat dilakukan dengan cara mengkonfigurasi secara manual tiap ukuran \emph{anchor box} atau dengan menggunakan algoritma \emph{clustering}.
  %Penggunaan algoritma \emph{clustering} akan lebih baik karena setiap ukuran \emph{anchor box}-nya disesuaikan dengan pengelompokan-pengelompokan ukuran \emph{bounding box} natural yang terdapat pada dataset.
  %Untuk penyesuaian saat training, dapat digunakan algoritma dari \textcite{anchoropt}.
  %Algoritma ini akan mengoptimisasi ukuran-ukuran anchor bukan hanya berdasarkan dataset, namun berdasarkan kemampuan dari neural network pendeteksi objeknya.
  %Untuk melakukan hal tersebut, algoritma ini akan memanfaatkan back propagation localization loss untuk rekalkulasi anchor.

  \subsection{Mosaic Augmentation}
  \label{section:mosaic_study}


  %Mosaic augmentation was introduced in Ultralytics' implementation of YOLOv3 \parencite{mosaic_aug}.
  %This augmentation technique will randomly pick 4 images of the dataset, and tile them randomly into one image like in \ref{fig:mosaic}.
  %It's called mosaic due to how the result of the augmented image have mosaic-like appereance.
  %\textcites{cspnet}{yolov4}{yolov5} reported increase in accuracy after applying mosaic augmentation.
  Mosaic augmentation was introduced in Ultralytics' implementation of YOLOv3 \parencite{mosaic_aug}. 
  %It is an effective technique for data augmentation in object detection.
  This method involves randomly selecting four images from the dataset and tiling them together into a single augmented image, as shown in Figure \ref{fig:mosaic}. 
  The resulting augmented image has a mosaic-like appearance, thus named "mosaic" augmentation.

  Several works \parencites{cspnet}{yolov4}{yolov5} have reported improvements in accuracy when utilizing mosaic augmentation in object detection tasks.
  By combining multiple images into one, mosaic augmentation provides the model with more diverse training samples, leading to enhanced performance in localizing and classifying objects.
  %Augmentasi mosaik merupakan teknik augmentasi yang baru dikenalkan pada YOLOv4.
  %Teknik augmentasi ini akan memilih 4 gambar dari dataset, memotong gambar-gambar tersebut dan menggabungkannya secara acak pada satu gambar seperti pada Gambar \ref{fig:mosaic}.
  %Hasil dari penggabungan itu membuat gambar terlihat seperti mosaik.
  %Teknik augmentasi ini mampu meningkatkan akurasi model \parencite{yolov4}.
  \captionsetup{belowskip=0pt}
  \begin{figure}[!hb]
    \centering
    \includegraphics[width=0.65\textwidth]{figures/mosaic-aug.png}
    \caption*{Source: \textcite{mosaic} under CC BY 4.0}
        \vspace{-2ex}
    \caption{Four example of mosaic augmentation}
    \label{fig:mosaic}
  \end{figure}


\section{Related Works}
\label{section:relatedwork}

  \subsection{Barunastra ITS' YOLOv4-tiny with added head}%Implementasi YOLOv4-tiny pada \emph{Autonomous Surface Vehicle}}

  \begin{figure}[p]
    \hfill%
    \begin{subfigure}[c][][c]{.45\textwidth}
        \includegraphics[width=1\linewidth]{figures/yolov4barun-regular.png}
        \caption{Regular YOLOv4-tiny prediction}
        \label{fig:barun-yolov4}
    \end{subfigure}\hfill  
    \begin{subfigure}[c][][c]{.45\textwidth}
        \includegraphics[width=1\linewidth]{figures/yolov4barun-addhead.png}
        \caption{YOLOv4-tiny with added head prediction}
        \label{fig:barun-yolov4-3l}
    \end{subfigure}\hfill%
    \caption*{Source: \textcite{barunastra}}
    \vspace{-1ex}
    \caption{Comparison of regular YOLOv4-tiny and YOLOv4-tiny with added head}
    \label{fig:barun}
  \end{figure}

  \textcite{barunastra} used YOLOv4-tiny as their Autonomous Surface Vehicle (ASV) object detector due to the computational
  device constraint.
  To detect objects that were at least 30 meters away from the ASV, they applied a modification of YOLOv4-tiny, which was YOLOv4-tiny
  but with additional head layer. The original YOLOv4-tiny only had 2 head layers, thus was only predicting in 2 scales.
  An addition of head layer allows it to predict in 3 scales. Using this modification, the network was able to detect small object
  better (as seen on Figure \ref{fig:barun}) and raised the overall mAP score by 4\% without significantly reducing latency.

  %\textcite{barunastra} menggunakan model modifikasi YOLOv4-tiny pada \emph{Autonomous Surface Vehicle}(ASV) mereka.
  %YOLOv4-tiny sebenarnya hanya menggunakan 2 layer head, namun yang diimplementasikan pada ASV adalah model YOLOv4-tiny
  %yang ditambahkan 1 layer head lagi sehingga menggunakan total sebanyak 3 layer head. Perubahan ini memberikan peningkatan
  %pada skor mAP dan memberikan kemampuan modelnya untuk mendeteksi objek yang lebih jauh.



  \subsection{exYOLO}
  \begin{figure}[p]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/exyolo.png}
    \caption*{Source: \textcite{exyolo} under CC BY-NC-ND 4.0 (see Appendix \ref{appendix:license})}
    \vspace{-1ex}
    \caption{Architecture of exYOLO}
    \label{fig:exyolo}
  \end{figure}

  exYOLO is a modification of YOLOv3 to detect small objects \parencite{exyolo}.
  \textcite{exyolo} thought that the features of small objects in an image would disappear
  after series stage of down-sampling in the neural network.
  To solve this, exYOLO added a feature enhancement before feature-fusion in the neck to one of the feature scale as seen in Figure \ref{fig:exyolo}.
  This change made exYOLO produce a higher mAP score on VOC2007 compared to its baseline YOLOv3.
    %exYOLO merupakan hasil modifikasi arsitektur YOLOv3 \parencite{exyolo}.
    %Pada exYOLO, dilakukan modifikasi \emph{neck} dengan menambahkan suatu \emph{Receptive Field Block} sebelum penggabungan \emph{feature map} yang akan diupsampling.
    %Modifikasi-modifikasi ini membuat exYOLO memiliki skor mAP yang lebih tinggi daripada YOLOv3 pada dataset PASCAL VOC 2007.

  \subsection{YOLO-Z}
  \begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{figures/yoloz-result.png}
    \caption*{Source: \textcite{yoloz} with permission (see Appendix \ref{appendix:license})}
    \vspace{-1ex}
    \caption{Small objects in the image. Comparison of YOLOZ-S and YOLOv5-S. YOLOv5-S was not able to detect the circled objects.}
    \label{fig:yolozcone}
  \end{figure}
  YOLO-Z is a derivative architecture of YOLOv5r5.0.
  This variant of YOLO modified the backbone, neck, and number of anchors of the original YOLOv5 to 
  enhance its capability of detecting small objects \parencite{yoloz}.
  These changes are backbone change from YOLOv5r5.0 to a down-scaled DenseNet,
  neck change from FPN to biFPN on some YOLO-Z scales, and increasing the number of anchors used at each scale.
  These changes successfully improve the small object detection capability as seen on figure \ref{fig:yolozcone}.

  YOLO-Z was aimed to be used in autonomous racing car. In this high speed environment, early detection
  of obstacle is crucial to plan for action. For that reason, the autonomous racing car must detect the cone-shaped obstacles
  that are far away from it. Since objects that are far away appear small on image captured by camera, YOLO-Z was designed with
  purpose of small object detection.
  %YOLO-Z merupakan arsitektur famili YOLO yang modifikasi dari YOLOv5 \parencite{yoloz}.
  %Modifikasi-modifikasi yang dilakukan meliputi pergantian \emph{backbone}, \emph{neck}, dan jumlah \emph{anchor}
  %\emph{Backbone} dari YOLOv5r5.0 menjadi DenseNet yang di-\emph{downscale}.
  %\emph{Neck} dari YOLO-Z juga diganti dari PanNet menjadi FPN dan biFPN tergatung pada \emph{scale} YOLO-Z yang digunakan.

  %Modifikasi pada YOLO-Z didesain untuk mendeteksi objek kecil untuk tujuan melakukan deteksi \emph{cone} yang nampak jauh pada lintasan \emph{autonomous racing} secara \emph{real time} (lihat Gambar \ref{fig:yolozcone}).
  %Modifikasi-modifikasi dibuktikan dapat meningkatkan kemampuan pendeteksian objek kecil \parencite{yoloz}.
  %Oleh karena itu, untuk meningkatkan kemampuan mendeteksi objek kecil YOLOv7, beberapa modifikasi yang dilakukan YOLO-Z pada YOLOv5 dapat diaplikasikan.